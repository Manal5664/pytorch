## PyTorch AutoGrad â€” Colab Notebook

This repository contains my **PyTorch practice notebook** created in **Google Colab**, focused on learning and experimenting with **AutoGrad** â€” PyTorchâ€™s automatic differentiation engine that powers neural network training.

---

### **Overview**

In this notebook, I explored how **PyTorch AutoGrad** works and performed several tasks related to **gradients and loss computation**.

The notebook covers:

- ğŸ”¹ Understanding how AutoGrad tracks computations  
- ğŸ”¹ Finding derivatives using `torch.autograd`  
- ğŸ”¹ Calculating gradients for tensors  
- ğŸ”¹ Computing **complex gradients**  
- ğŸ”¹ Defining and computing a **loss function**  
- ğŸ”¹ Implementing **gradient descent manually using AutoGrad**  
- ğŸ”¹ Visualizing **computation graphs**  
- ğŸ”¹ Comparing **AutoGrad with manual differentiation**  

---

### âš™ï¸ **Environment**

- **Platform:** Google Colab  
- **Framework:** PyTorch  
- **Language:** Python  

---

### **Key Takeaway**

Through this notebook, I learned how **PyTorch automatically computes gradients**, manages the computational graph, implements loss and optimization, and how AutoGrad simplifies training deep learning models.
